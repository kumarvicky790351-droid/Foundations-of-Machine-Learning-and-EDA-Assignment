{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the difference between AI, ML, DL, and Data Science? Provide a brief explanation of each.\n",
        "\n",
        "* Artificial Intelligence (AI): This is the broadest concept, referring to the simulation of human intelligence in machines that are programmed to think and learn like humans. Its scope is vast, encompassing any technique that enables computers to mimic human intelligence. Techniques include search algorithms, logic, and knowledge representation. Applications are everywhere, from virtual assistants and self-driving cars to medical diagnosis and game playing.\n",
        "\n",
        "* Machine Learning (ML): A subset of AI that focuses on enabling systems to learn from data and improve over time without being explicitly programmed. Its scope is specifically about learning from data. Techniques include supervised learning (e.g., regression, classification), unsupervised learning (e.g., clustering), and reinforcement learning. Applications include spam detection, recommendation systems, and fraud detection.\n",
        "\n",
        "* Deep Learning (DL): A subfield of Machine Learning that uses artificial neural networks with multiple layers (hence \"deep\") to model complex patterns in data. Its scope is centered on using deep neural networks for learning. Techniques involve various neural network architectures like Convolutional Neural Networks (CNNs) for images and Recurrent Neural Networks (RNNs) for sequential data. Applications include image and speech recognition, natural language processing, and drug discovery.\n",
        "\n",
        "* Data Science: An interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. Its scope is broader than just ML and DL, involving the entire process of data collection, cleaning, analysis, visualization, and interpretation. Techniques draw from statistics, mathematics, computer science, and domain expertise. Applications are widespread across industries for decision-making, trend analysis, and predictive modeling.\n",
        "\n",
        "In essence:\n",
        "\n",
        "* AI is the overarching goal of creating intelligent machines.\n",
        "* ML is one way to achieve AI by learning from data.\n",
        "* DL is a specific type of ML that uses deep neural networks.\n",
        "* Data Science is the field that uses various techniques, including ML and DL, to extract insights from data."
      ],
      "metadata": {
        "id": "5BJgs0caL5lI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2. Explain overfitting and underfitting in ML. How can you detect and prevent them?\n",
        "\n",
        "Overfitting: This occurs when a model learns the training data too well, including the noise and outliers. As a result, the model performs very well on the training data but poorly on unseen or new data.\n",
        "\n",
        "* Detection: High accuracy on training data, but significantly lower accuracy on validation or test data.\n",
        "* Prevention:\n",
        "  * More data: Increasing the amount of training data can help the model generalize better.\n",
        "  * Feature selection: Removing irrelevant or redundant features can reduce complexity.\n",
        "  * Regularization: Techniques like L1 and L2 regularization add a penalty to the model's complexity, discouraging large coefficients.\n",
        "  * Cross-validation: Using techniques like k-fold cross-validation helps evaluate the model's performance on different subsets of the data, providing a more reliable estimate of its generalization ability.\n",
        "  * Simplifying the model: Using a simpler model with fewer parameters can reduce the risk of overfitting.\n",
        "  * Dropout (in neural networks): Randomly dropping out neurons during training prevents the network from becoming too reliant on specific connections.\n",
        "\n",
        "* Underfitting: This occurs when a model is too simple to capture the underlying patterns in the training data. It performs poorly on both the training data and unseen data.\n",
        "\n",
        " * Detection: Low accuracy on both training and validation/test data.\n",
        " * Prevention:\n",
        "   * More complex model: Using a more complex model with more parameters or layers (e.g., a higher degree polynomial for regression, or a deeper neural network).\n",
        "   * More features: Adding more relevant features to the dataset can help the model capture more patterns.\n",
        "   * Reducing regularization: If regularization was applied, reducing its strength can allow the model to fit the data more closely.\n",
        "   * Increasing training time: For iterative algorithms, training for more epochs might be necessary (though be careful not to overfit).\n",
        "* Bias-Variance Tradeoff:\n",
        "\n",
        "  * Bias: The error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias leads to underfitting.\n",
        "  * Variance: The sensitivity of the model to the specific training data. High variance leads to overfitting.\n",
        "  * The bias-variance tradeoff is a fundamental concept in ML. We aim to find a balance between bias and variance to achieve good generalization performance. Increasing model complexity typically decreases bias but increases variance, and vice versa. Regularization and cross-validation are techniques used to manage this tradeoff."
      ],
      "metadata": {
        "id": "fxeC8W-HPoAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3. How would you handle missing values in a dataset? Explain at least three methods with examples.\n",
        "\n",
        "Handling missing values is a crucial step in data preprocessing. Here are three common methods:\n",
        "\n",
        "1. Deletion:\n",
        "\n",
        "* Explanation: This involves removing rows or columns that contain missing values.\n",
        "* When to use: This method is suitable when the percentage of missing data is small and the missingness is not concentrated in specific rows or columns that are important for the analysis.\n",
        "* Examples:\n",
        "  * Deleting rows: If only a few rows have missing values in a large dataset, you can drop those rows using pandas' dropna() function.\n",
        "  * Deleting columns: If a column has a very high percentage of missing values and is not critical for the analysis, you can drop the entire column.\n",
        "* Caveats: Deletion can lead to a significant loss of data, which can be problematic if the dataset is small or the missingness is not random.\n",
        "\n",
        "Mean/Median/Mode Imputation:\n",
        "\n",
        "* Explanation: This involves replacing missing values with the mean, median, or mode of the non-missing values in the respective column.\n",
        "* When to use: This method is simple and quick. Mean imputation is suitable for numerical data with a normal distribution, while median imputation is more robust to outliers. Mode imputation is used for categorical data.\n",
        "* Examples:\n",
        "  * Mean imputation: Replace missing age values with the average age of the non-missing entries.\n",
        "  * Median imputation: Replace missing income values with the median income to be less affected by extreme values.\n",
        "  * Mode imputation: Replace missing city values with the most frequent city in the dataset.\n",
        "* Caveats: Imputation with a single value can distort the original distribution of the data and reduce variance. It also doesn't account for the relationships between variables.\n",
        "\n",
        "3. Predictive Modeling (e.g., using K-Nearest Neighbors or Regression):\n",
        "\n",
        "* Explanation: This method involves building a model to predict the missing values based on the other variables in the dataset.\n",
        "* When to use: This is a more sophisticated method that can capture relationships between variables and provide more accurate imputations, especially when the missingness is not random.\n",
        "* Examples:\n",
        "  * K-Nearest Neighbors (KNN) imputation: For a missing age value, find the k-nearest neighbors (based on other features) of the data point with the missing value and impute the age based on the ages of those neighbors (e.g., average age of neighbors).\n",
        "  * Regression imputation: If you have missing values in a numerical variable, you can build a regression model using other variables as predictors to estimate the missing values.\n",
        "* Caveats: This method is more computationally expensive than simpler methods. The accuracy of the imputation depends on the quality of the model and the relationships between variables.\n",
        "\n",
        "The choice of method for handling missing values depends on the nature of the data, the amount of missingness, and the goals of the analysis. It's often a good practice to explore the patterns of missingness before deciding on a method."
      ],
      "metadata": {
        "id": "9D_QyfhfUmmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4.  What is an imbalanced dataset? Describe two techniques to handle it (theoretical + practical).\n",
        "\n",
        "An imbalanced dataset is one where the distribution of the target variable's classes is not equal. This means that some classes have a significantly larger number of instances than others. This can be a problem for machine learning models, as they may become biased towards the majority class and perform poorly on the minority class.\n",
        "\n",
        "Here are two techniques to handle imbalanced datasets:\n",
        " 1. Resampling Techniques: These techniques involve changing the distribution of the dataset to make the classes more balanced.\n",
        "   * Oversampling: This involves increasing the number of instances in the minority class.\n",
        "     * Theoretical: You can randomly duplicate instances from the minority class or use more sophisticated methods like SMOTE (Synthetic Minority Over-sampling Technique). SMOTE creates synthetic instances of the minority class by interpolating between existing minority class instances.\n",
        "     * Practical Example (using SMOTE with Python):"
      ],
      "metadata": {
        "id": "59UNZX9eWrtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Create a sample imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2,\n",
        "weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0,\n",
        "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
        "\n",
        "print(f\"Original dataset shape: {Counter(y)}\")\n",
        "\n",
        "# Apply Random Under-sampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "\n",
        "print(f\"Resampled dataset shape: {Counter(y_res)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn6lMVHiXTW0",
        "outputId": "014cb452-0596-45ad-c9bc-daa5eef36ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: Counter({np.int64(0): 900, np.int64(1): 100})\n",
            "Resampled dataset shape: Counter({np.int64(0): 100, np.int64(1): 100})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create a sample imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, class_sep=2,\n",
        "weights=[0.9, 0.1], n_informative=3, n_redundant=1, flip_y=0,\n",
        "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model without class weights\n",
        "model_no_weights = LogisticRegression()\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "print(\"Classification Report (without class weights):\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Train a logistic regression model with class weights\n",
        "# 'balanced' automatically adjusts weights inversely proportional to class frequencies\n",
        "model_with_weights = LogisticRegression(class_weight='balanced')\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "print(\"\\nClassification Report (with class weights):\")\n",
        "print(classification_report(y_test, y_pred_with_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkQlXY_aXY_B",
        "outputId": "e8885da9-640c-4762-bf1c-10cd0c82d7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (without class weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       181\n",
            "           1       0.95      1.00      0.97        19\n",
            "\n",
            "    accuracy                           0.99       200\n",
            "   macro avg       0.97      1.00      0.99       200\n",
            "weighted avg       1.00      0.99      1.00       200\n",
            "\n",
            "\n",
            "Classification Report (with class weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       181\n",
            "           1       0.90      1.00      0.95        19\n",
            "\n",
            "    accuracy                           0.99       200\n",
            "   macro avg       0.95      0.99      0.97       200\n",
            "weighted avg       0.99      0.99      0.99       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5. Why is feature scaling important in ML? Compare Min-Max scaling and Standardization.\n",
        "\n",
        "Feature scaling is a crucial preprocessing step in Machine Learning, especially for algorithms that are sensitive to the scale of the input features. Here's why it's important and a comparison of two common methods: Min-Max scaling and Standardization.\n",
        "\n",
        "Why is Feature Scaling Important?\n",
        "\n",
        "Many machine learning algorithms, particularly those that rely on distance calculations or gradient descent, are significantly affected by the scale of the features.\n",
        "\n",
        "* Distance-based algorithms (e.g., K-Nearest Neighbors (KNN), Support Vector Machines (SVM) with RBF kernel): These algorithms calculate distances between data points. If features have different scales, features with larger values will dominate the distance calculation, effectively ignoring features with smaller values. Scaling ensures that all features contribute equally to the distance metric.\n",
        "* Gradient Descent based algorithms (e.g., Linear Regression, Logistic Regression, Neural Networks): Gradient descent algorithms find the minimum of a cost function by iteratively updating model parameters. If features are not scaled, the cost function will have elongated or stretched contours, making the optimization process slower and potentially causing the algorithm to oscillate and converge slowly or not converge at all. Scaling makes the contours more spherical, leading to faster and more stable convergence.\n",
        "* Regularization techniques (e.g., L1 and L2 regularization): These techniques add a penalty to the magnitude of the model coefficients. If features are not scaled, the penalty will disproportionately affect coefficients associated with features that have larger values, regardless of their actual importance. Scaling ensures that the regularization penalty is applied fairly to all coefficients.\n",
        "\n",
        "Comparison of Min-Max Scaling and Standardization:\n",
        "\n",
        "Here's a comparison of two popular feature scaling techniques:\n",
        "\n",
        "1. Min-Max Scaling (Normalization):\n",
        " * Explanation: This technique scales features to a fixed range, usually between 0 and 1.\n",
        " * Pros:\n",
        "   * Easy to understand and implement.\n",
        "   * Scales data to a specific range, which can be useful for algorithms that require inputs in a certain range (e.g., neural networks with sigmoid activation functions).\n",
        "* Cons:\n",
        "  * Sensitive to outliers: Outliers can significantly affect the minimum and maximum values, leading to a distorted scaled distribution.\n",
        "  * Compresses the range of values: If the original data has a wide range, Min-Max scaling can compress the values, potentially losing some information about the relative distances between data points.\n",
        "\n",
        "2. Standardization (Z-score normalization):\n",
        "  * Explanation: This technique scales features to have a mean of 0 and a standard deviation of 1.\n",
        "  * Pros:\n",
        "    * Less affected by outliers: Standardization uses the mean and standard deviation, which are less sensitive to extreme values compared to the minimum and maximum.\n",
        "    * Preserves the shape of the original distribution: Standardization centers the data around 0 but does not change the shape of the distribution.\n",
        "  * Cons:\n",
        "     * The scaled values do not have a fixed range, which might be a concern for some algorithms.\n",
        "\n",
        "Which one to use?\n",
        "\n",
        "The choice between Min-Max scaling and Standardization depends on the specific algorithm and the nature of the data:\n",
        "\n",
        "* Standardization is generally preferred for algorithms that assume a Gaussian distribution or are sensitive to outliers (e.g., Logistic Regression, Linear Regression, SVM, K-Means).\n",
        "* Min-Max scaling is often used when you need to scale data to a specific range (e.g., for image processing or when using neural networks with certain activation functions).\n",
        "\n",
        "It's often a good practice to try both and evaluate which one performs better for your specific task."
      ],
      "metadata": {
        "id": "Ayvl1vn9YHCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6. Compare Label Encoding and One-Hot Encoding. When would you prefer one over the other ?\n",
        "\n",
        "When dealing with categorical variables in machine learning, we need to convert them into a numerical format that models can understand. Two common techniques for this are Label Encoding and One-Hot Encoding.\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "* Explanation: Label Encoding assigns a unique integer to each category in a column. For example, if you have a \"color\" column with categories \"Red\", \"Green\", and \"Blue\", Label Encoding might assign 0 to \"Red\", 1 to \"Green\", and 2 to \"Blue\".\n",
        "* How it works: It simply maps each unique category to a numerical label.\n",
        "* When to use: Label Encoding is suitable for ordinal categorical variables, where there is an inherent order or ranking among the categories (e.g., \"Small\", \"Medium\", \"Large\"). In this case, the numerical order reflects the categorical order.\n",
        "* Caveats: If you use Label Encoding on nominal categorical variables (where there is no inherent order, like colors), the model might incorrectly interpret the numerical order as a ranking, which can lead to biased or incorrect results.\n",
        "\n",
        "One-Hot Encoding:\n",
        "\n",
        "* Explanation: One-Hot Encoding creates new binary columns for each category in a categorical feature. For example, for the \"color\" column with \"Red\", \"Green\", and \"Blue\", it would create three new columns: \"Color_Red\", \"Color_Green\", and \"Color_Blue\". For a data point where the color is \"Red\", the \"Color_Red\" column would have a value of 1, and the other color columns would have a value of 0.\n",
        "* How it works: It represents each category as a binary vector where only one element is 1 (indicating the presence of that category) and the rest are 0.\n",
        "* When to use: One-Hot Encoding is generally preferred for nominal categorical variables, where there is no inherent order among the categories. It avoids creating the false sense of order that Label Encoding introduces for nominal data.\n",
        "* Caveats: One-Hot Encoding can lead to a significant increase in the number of features, especially if a categorical variable has many unique categories. This can result in a sparse dataset and the \"curse of dimensionality,\" which can impact model performance and increase computational cost. Additionally, it can introduce multicollinearity if all dummy variables are kept, which can be an issue for some models (this is often handled by dropping one of the dummy variables).\n",
        "\n",
        "Summary of Preference:\n",
        "\n",
        "* Use Label Encoding for ordinal categorical variables where the numerical order reflects a meaningful ranking.\n",
        "* Use One-Hot Encoding for nominal categorical variables to avoid implying an incorrect order."
      ],
      "metadata": {
        "id": "wZe8KdWVAJvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7. Google Play Store Dataset\n",
        "\n",
        "a). Analyze the relationship between app categories and ratings. Which categories have the\n",
        "highest/lowest average ratings, and what could be the possible reasons?\n"
      ],
      "metadata": {
        "id": "9AKR58RaBelV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset\n",
        "Subtask:\n",
        "\n",
        "Load the googleplaystore.csv file into a pandas DataFrame.\n",
        "\n",
        "Reasoning: Load the data into a pandas DataFrame and display the first few rows."
      ],
      "metadata": {
        "id": "pvJ_d-nPB6ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/googleplaystore.csv')\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "tjRjzHkNngAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Column           | Description                                          |\n",
        "| ---------------- | ---------------------------------------------------- |\n",
        "| `App`            | App name                                             |\n",
        "| `Category`       | App category (e.g., GAME, BUSINESS, EDUCATION, etc.) |\n",
        "| `Rating`         | Average user rating (1.0–5.0)                        |\n",
        "| `Reviews`        | Number of user reviews                               |\n",
        "| `Installs`       | Number of installs                                   |\n",
        "| `Type`           | Free or Paid                                         |\n",
        "| `Price`          | App price                                            |\n",
        "| `Content Rating` | Age appropriateness (Everyone, Teen, etc.)           |\n",
        "| `Genres`         | More detailed classification                         |\n",
        "| `Last Updated`   | Last update date                                     |\n",
        "\n"
      ],
      "metadata": {
        "id": "d2m5T0i2nk2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the data\n",
        "Subtask:\n",
        "\n",
        "Handle missing values and convert the 'Rating' column to a numeric type.\n",
        "\n",
        "Reasoning: Handle missing values in the 'Rating' column and convert it to a numeric type as instructed."
      ],
      "metadata": {
        "id": "jDNm1CrJr3rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['Rating'], inplace=True)\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')"
      ],
      "metadata": {
        "id": "OpOYeGuTr2Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze category ratings\n",
        "Subtask:\n",
        "Group the data by 'Category' and calculate the mean 'Rating' for each category.\n",
        "\n",
        "Reasoning: Group the DataFrame by 'Category' and calculate the mean of the 'Rating' column for each category to find the average rating per category."
      ],
      "metadata": {
        "id": "5zF_4mXssMxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_category_ratings = df.groupby('Category')['Rating'].mean()\n",
        "display(average_category_ratings)"
      ],
      "metadata": {
        "id": "rNFthl5Rsi-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Category            | Avg. Rating | Observation                                                                 |\n",
        "| ------------------- | ----------- | --------------------------------------------------------------------------- |\n",
        "| EDUCATION           | ⭐ 4.36      | High satisfaction — educational apps are often simple, targeted, and niche. |\n",
        "| BOOKS_AND_REFERENCE | ⭐ 4.35      | Informational apps; fewer bugs and stable functionality.                    |\n",
        "| ART_AND_DESIGN      | ⭐ 4.34      | Visually appealing, creative user base.                                     |\n",
        "| EVENTS              | ⭐ 4.32      | Simple apps with limited features.                                          |\n",
        "| HEALTH_AND_FITNESS  | ⭐ 4.28      | Positive user engagement and niche interest.                                |\n",
        "| ENTERTAINMENT       | ⭐ 4.22      | Engaging, but can vary by audience.                                         |\n",
        "| GAME                | ⭐ 4.19      | Popular, but mixed reviews due to ads and in-app purchases.                 |\n",
        "| TOOLS               | ⭐ 4.07      | Essential apps, but technical issues may reduce ratings.                    |\n",
        "| COMMUNICATION       | ⭐ 4.06      | Apps like messengers — heavy usage, frequent bugs.                          |\n",
        "| SOCIAL              | ⭐ 4.02      | Subjective user experiences, frequent updates cause dissatisfaction.        |\n",
        "| FINANCE             | ⭐ 4.00      | Users are sensitive to reliability and trust issues.                        |\n",
        "| DATING              | ⭐ 3.97      | User experience highly variable; expectations often unmet.                  |\n",
        "| NEWS_AND_MAGAZINES  | ⭐ 3.95      | Content quality varies; sometimes poor design.                              |\n"
      ],
      "metadata": {
        "id": "DjXr-c75wPWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization\n",
        "\n",
        "A bar chart or boxplot can show the distribution:"
      ],
      "metadata": {
        "id": "rfVS3-VOvyyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Rating', y='Category', data=df, estimator='mean', ci=None, order=category_ratings.index)\n",
        "plt.title('Average Rating by App Category')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.ylabel('Category')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UQs8haZZwiTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights & Possible Reasons\n",
        "\n",
        "| High-Rating Categories                         | Reasons                                                                                             |\n",
        "| ---------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
        "| **Education, Books & Reference, Art & Design** | Simple interfaces, fewer bugs, targeted audiences, and clear purpose. Users download intentionally. |\n",
        "| **Health & Fitness, Events**                   | Specialized, utility-based apps with loyal users and limited negative experiences.                  |\n",
        "\n",
        "\n",
        "| Low-Rating Categories       | Reasons                                                                  |\n",
        "| --------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Social, Dating, Finance** | High user expectations, privacy/trust concerns, or poor user experience. |\n",
        "| **Communication, Tools**    | Technical instability, battery/data usage, frequent ads.                 |\n",
        "| **Games**                   | Mixed reviews due to ads, difficulty, or pay-to-win features.            |\n"
      ],
      "metadata": {
        "id": "b95JQJgjwplT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "* Highest Average Ratings: Education, Books & Reference, Art & Design.\n",
        "→ Focused purpose, niche users, fewer ads, and stable functionality.\n",
        "\n",
        "* Lowest Average Ratings: Dating, Social, Finance, Tools.\n",
        "→ High user expectations, technical challenges, and subjective satisfaction."
      ],
      "metadata": {
        "id": "_NXQERbexAm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8. Titanic Dataset\n",
        "\n",
        "a) Compare the survival rates based on passenger class (Pclass). Which class had the highest\n",
        "survival rate, and why do you think that happened?\n",
        "\n",
        "b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and\n",
        "adults (Age ≥ 18). Did children have a better chance of survival?\n",
        "\n"
      ],
      "metadata": {
        "id": "OacPjZv8xI0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libreries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3Uj0CkECYAe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df_titanic = pd.read_csv('DataSet/titanic.csv')"
      ],
      "metadata": {
        "id": "HncFG4qkYDhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows\n",
        "print(\"Dataset Preview:\")\n",
        "df_titanic.head()"
      ],
      "metadata": {
        "id": "R7QUq7qdYEsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "df_titanic.isnull().sum()"
      ],
      "metadata": {
        "id": "lJMC8b0pYH4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing Age or Pclass or Survived\n",
        "df_titanic = df_titanic.dropna(subset=['Age', 'Pclass', 'Survived'])"
      ],
      "metadata": {
        "id": "y7rhyZj6YWFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Compare survival rates by Passenger Class (Pclass)\n",
        "survival_by_class = df_titanic.groupby('Pclass')['Survived'].mean() * 100\n",
        "print(\"\\nSurvival Rate by Passenger Class (%):\")\n",
        "survival_by_class"
      ],
      "metadata": {
        "id": "UduZc93dYaAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot survival by class\n",
        "plt.figure(figsize=(8,6))\n",
        "survival_by_class.plot(kind='bar', color=['gold','silver', 'brown'], edgecolor='black')\n",
        "plt.title('Survival Rate by Passenger Class')\n",
        "plt.xlabel('Passenger Class (1 = Upper, 2 = Middle, 3 = Lower)')\n",
        "plt.ylabel('Survival Rate (%)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e5efoalQYfbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Analyze survival by Age Group (Children vs Adults)\n",
        "df_titanic['AgeGroup'] = df_titanic['Age'].apply(lambda x: 'Child' if x < 18 else 'Adult')\n",
        "\n",
        "survival_by_agegroup = df_titanic.groupby('AgeGroup')['Survived'].mean() * 100\n",
        "print(\"\\nSurvival Rate by Age Group (%):\")\n",
        "survival_by_agegroup"
      ],
      "metadata": {
        "id": "s8d_JlrwY9Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot survival by age group\n",
        "plt.figure(figsize=(5,4))\n",
        "survival_by_agegroup.plot(kind='bar', color=['lightblue', 'orange'], edgecolor='black')\n",
        "plt.title('Survival Rate by Age Group')\n",
        "plt.xlabel('Age Group')\n",
        "plt.ylabel('Survival Rate (%)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bCkRT0aXZALh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the Analysis**\n",
        "\n",
        "**🎟️ 1. Passenger Class vs Survival**\n",
        "\n",
        "* **1st Class** passengers have the **highest survival rate** (typically around **60–65%**).\n",
        "\n",
        "* **3rd Class** passengers have the **lowest survival rate** (often around **20–25%**).\n",
        "\n",
        "**Reasons:**\n",
        "\n",
        "* 1st class passengers were closer to the lifeboats (upper decks).\n",
        "\n",
        "* Crew prioritized wealthy and high-status passengers during evacuation.\n",
        "\n",
        "* 3rd class passengers were located in lower decks, far from exits, and often delayed in reaching lifeboats.\n",
        "\n",
        "**👶 2. Age (Children vs Adults)**\n",
        "\n",
        "* **Children (<18 years)** had a **higher survival rate** than adults.\n",
        "\n",
        "* This aligns with the “Women and Children First” policy followed during evacuation.\n",
        "\n",
        "Reasons:**\n",
        "\n",
        "* Crew gave priority to saving children and women.\n",
        "\n",
        "* Many children in upper-class families had better access to lifeboats.\n",
        "\n",
        "* Adult males had the lowest survival rate due to giving up lifeboat seats.\n",
        "\n",
        "**💡 Conclusion**\n",
        "* **1st Class** → Highest survival rate (more access & priority).\n",
        "* **Children (<18)** → Higher survival rate (evacuation priority).\n",
        "* **3rd Class & Adults** → Lowest survival rate (limited access, slower evacuation).\n",
        "---"
      ],
      "metadata": {
        "id": "t2VxZLZ0ZFB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9.  Flight Price Prediction Dataset\n",
        "\n",
        "1. How do flight prices vary with the days left until departure? Identify any exponential price surges and recommend the best booking window.\n",
        "\n",
        "2. Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are consistently cheaper/premium, and why?\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git"
      ],
      "metadata": {
        "id": "WxRkkbCIZXS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libreries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "hmKbEseZZfEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df_fp = pd.read_excel('DataSet/flight_price.xlsx')"
      ],
      "metadata": {
        "id": "EgH-TeGlZj5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows\n",
        "print(\"Dataset Preview:\")\n",
        "df_fp.head()"
      ],
      "metadata": {
        "id": "IgEYVq2FZlx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "df_fp.isnull().sum()"
      ],
      "metadata": {
        "id": "ojZuUpCaZokl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fp['Date_of_Journey'] = pd.to_datetime(df_fp['Date_of_Journey'], format='%d/%m/%Y')\n",
        "today = datetime.now().date()\n",
        "df_fp['Days_Left'] = (df_fp['Date_of_Journey'].dt.date - today).apply(lambda x: x.days)"
      ],
      "metadata": {
        "id": "RhmUNbxwZqkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by days left and calculate average price\n",
        "price_by_days = df_fp.groupby('Days_Left')['Price'].mean().reset_index().sort_values(by='Days_Left')"
      ],
      "metadata": {
        "id": "tX9R6ROyZuGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_by_days.head()"
      ],
      "metadata": {
        "id": "MqVdNZ9_Zxk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_by_days.tail()"
      ],
      "metadata": {
        "id": "eJ72VDqLZz51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-4: Line plot for average price vs. days left\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=price_by_days, x='Days_Left', y='Price')\n",
        "plt.title('Average Flight Price vs. Days Left Until Departure')\n",
        "plt.xlabel('Days Left Until Departure')\n",
        "plt.ylabel('Average Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i1nBsZEwapO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify exponential price surges\n",
        "\n",
        "df_delhi_mumbai = df_fp[(df_fp['Source'] == 'Delhi') & (df_fp['Destination'] == 'Mumbai')]\n",
        "avg_price_by_airline = df_delhi_mumbai.groupby('Airline')['Price'].mean().reset_index().sort_values(by='Price')\n",
        "\n",
        "avg_price_by_airline"
      ],
      "metadata": {
        "id": "CFwRwQ3-avNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fp['Source'].unique()"
      ],
      "metadata": {
        "id": "_WlHrz7aaxUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fp['Destination'].unique()"
      ],
      "metadata": {
        "id": "BJHT7G1JbH_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_fp.groupby(['Source', 'Destination']).size().reset_index(name='count'))"
      ],
      "metadata": {
        "id": "uksGDDK7bKc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-9: Bar plot for average price by airline on Delhi-Mubai route\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=avg_price_by_airline, x='Airline', y='Price', palette='viridis')\n",
        "plt.title('Average Flight Prices Across Airlines (Delhi → Mumbai)')\n",
        "plt.xlabel('Airline')\n",
        "plt.ylabel('Average Price (₹)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "04DWsv6Ubd6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10. HR Analytics Dataset.\n",
        "\n",
        "1. What factors most strongly correlate with employee attrition? Use visualizations to show key drivers (e.g., satisfaction, overtime, salary).\n",
        "\n",
        "2. Are employees with more projects more likely to leave?\n",
        "\n",
        "\n",
        "\n",
        "Dataset Overview\n",
        "\n",
        "A typical HR Analytics dataset includes variables like:\n",
        "\n",
        "\n",
        "| Column                  | Description                                    |\n",
        "| ----------------------- | ---------------------------------------------- |\n",
        "| `satisfaction_level`    | Employee satisfaction (0–1)                    |\n",
        "| `last_evaluation`       | Last performance review (0–1)                  |\n",
        "| `number_project`        | Number of projects the employee worked on      |\n",
        "| `average_montly_hours`  | Average monthly hours                          |\n",
        "| `time_spend_company`    | Years spent in the company                     |\n",
        "| `Work_accident`         | Whether the employee had a work accident       |\n",
        "| `promotion_last_5years` | Whether promoted in last 5 years               |\n",
        "| `salary`                | Categorical: low, medium, high                 |\n",
        "| `department`            | Department/field                               |\n",
        "| `left`                  | Target variable (1 = left company, 0 = stayed) |\n",
        "\n",
        "\n",
        "1. Key Correlations with Attrition\n",
        "\n",
        "To find what factors correlate with attrition (left), we can compute correlation coefficients or visualize distributions.\n",
        "\n",
        "Example (Correlation Matrix Heatmap)"
      ],
      "metadata": {
        "id": "Aivcnk9bbkLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr = df.corr(numeric_only=True)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix with Attrition')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M3CYZ3oodFMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typical Findings (from known analyses)\n",
        "\n",
        "| Factor                    | Correlation with Attrition | Insight                                                             |\n",
        "| ------------------------- | -------------------------- | ------------------------------------------------------------------- |\n",
        "| **satisfaction_level**    | **−0.39**                  | Lower satisfaction → higher attrition                               |\n",
        "| **number_project**        | **+0.35 (nonlinear)**      | Moderate projects (3–4) retain employees; very high (6–7) → burnout |\n",
        "| **average_montly_hours**  | **+0.34**                  | More hours → more attrition (overtime fatigue)                      |\n",
        "| **time_spend_company**    | **+0.14**                  | Longer tenure → more likely to leave (career stagnation)            |\n",
        "| **last_evaluation**       | **+0.28 (nonlinear)**      | Both very low and very high performers tend to leave                |\n",
        "| **salary (encoded)**      | **−0.25**                  | Lower salary → higher attrition                                     |\n",
        "| **promotion_last_5years** | **−0.07**                  | Lack of promotion slightly increases attrition                      |\n",
        "\n",
        "\n",
        "\n",
        "2. Visualizations — Key Drivers\n",
        "\n",
        "(a) Satisfaction Level vs Attrition\n",
        "\n",
        "Employees who left generally have lower satisfaction."
      ],
      "metadata": {
        "id": "FksdsCf0dK8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data=df, x='satisfaction_level', hue='left', fill=True)\n",
        "plt.title('Satisfaction Level vs Attrition')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aEo3-PUUde1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: A strong peak around satisfaction ≈ 0.4 for employees who left."
      ],
      "metadata": {
        "id": "fPvxL5Q0djJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Average Monthly Hours vs Attrition\n",
        "\n",
        "Employees who work much longer hours (200+) are more likely to leave."
      ],
      "metadata": {
        "id": "5RHFnJ0qdtIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x='left', y='average_montly_hours', data=df)\n",
        "plt.title('Average Monthly Hours vs Attrition')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xhYxPb0CdzkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Overworked employees tend to quit more.\n",
        "\n",
        "(c) Salary vs Attrition"
      ],
      "metadata": {
        "id": "O0mdM6mwd4ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='salary', hue='left', data=df, order=['low','medium','high'])\n",
        "plt.title('Attrition by Salary Level')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xgD6JTyneQzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Most leavers come from low salary group.\n",
        "\n",
        "(d) Promotion vs Attrition"
      ],
      "metadata": {
        "id": "WXc9EBmAeVBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='promotion_last_5years', y='left', data=df)\n",
        "plt.title('Promotion History vs Attrition Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iiXAD75beYhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Employees without promotion are more likely to leave.\n",
        "\n",
        "3. Are Employees with More Projects More Likely to Leave?\n",
        "\n",
        "Let’s visualize:"
      ],
      "metadata": {
        "id": "T_NvK_YLe2JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='number_project', hue='left', data=df)\n",
        "plt.title('Attrition by Number of Projects')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DS-1rbr7ftrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "Employees with 2 or fewer projects often leave due to boredom or lack of engagement.\n",
        "\n",
        "Employees with 6 or 7 projects leave due to overload and burnout.\n",
        "\n",
        "Employees with 3–4 projects have the lowest attrition (healthy workload).\n",
        "\n",
        "✅ So, the relationship is nonlinear (U-shaped) — both underworked and overworked employees are more likely to leave.\n",
        "\n",
        "Summary of Insights\n",
        "| Factor                           | Effect on Attrition | Explanation                     |\n",
        "| -------------------------------- | ------------------- | ------------------------------- |\n",
        "| **Low satisfaction**             | ⬆️ Attrition        | Dissatisfaction drives turnover |\n",
        "| **High workload / overtime**     | ⬆️ Attrition        | Burnout and stress              |\n",
        "| **Low salary**                   | ⬆️ Attrition        | Compensation dissatisfaction    |\n",
        "| **No promotion**                 | ⬆️ Attrition        | Career stagnation               |\n",
        "| **Too few or too many projects** | ⬆️ Attrition        | Under-challenged or overworked  |\n",
        "\n",
        "\n",
        "Key : -\n",
        "\n",
        "Satisfaction is the strongest predictor.\n",
        "\n",
        "Overtime and high project counts lead to burnout.\n",
        "\n",
        "Low pay and no promotion discourage retention.\n",
        "\n",
        "Optimal conditions: medium workload, fair salary, career growth, and recognition."
      ],
      "metadata": {
        "id": "6EUcDKb7fw_8"
      }
    }
  ]
}